{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUMHsRFaBot_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmSM2hz3B70S"
      },
      "source": [
        "# RDD\n",
        "\n",
        "RDD (Resilient Distributed Dataset) is a fundamental building block of PySpark which is fault-tolerant, immutable distributed collections of objects. Immutable meaning once you create an RDD you cannot change it. Each record in RDD is divided into logical partitions, which can be computed on different nodes of the cluster.\n",
        "\n",
        "# Adavantages of RDD\n",
        "\n",
        "1.Immutablity:\n",
        "PySpark RDD’s are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
        "\n",
        "2.In-memeory computation:\n",
        "PySpark loads the data from disk and process in memory and keeps the data in memory, this is the main difference between PySpark and Mapreduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.\n",
        "\n",
        "3.Fault tolerant:\n",
        "PySpark RDD’s are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
        "\n",
        "4.Lazy evaluation:\n",
        "PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
        "\n",
        "5.Partitioning:\n",
        "When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available.\n",
        "\n",
        "\n",
        "# Limitations of RDD\n",
        "\n",
        "PySpark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application. For these applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases. The goal of RDD is to provide an efficient programming model for batch analytics and leave these asynchronous applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G2gsgdkEKan"
      },
      "source": [
        "# Create RDD using sparkContext.parallelize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDnEIJGtB-Oe",
        "outputId": "aad16907-548b-44f7-f47d-c80825c6b7eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=7241c0a75304d7e5fc318c5dd27d4fcdfa23aefc5912bda6259fa6e05c215f82\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "houX0-ZzEPYy"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark  = SparkSession.builder.appName('shivashankar').master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZomJhq8F68I",
        "outputId": "db37136a-5888-485a-8d86-4deb0bcffda5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data = list(range(1,11))\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HRqCCpzCGMne"
      },
      "outputs": [],
      "source": [
        "rdd = spark.sparkContext.parallelize(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFf2tSNdGVBG",
        "outputId": "e1e51bbb-97d8-4cb2-e090-becfde016222"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "rdd.glom().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcBpfv_8GXcP",
        "outputId": "0c0960b0-0621-48dc-fd5c-f6f447418cf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O6ViHAMPGeju"
      },
      "outputs": [],
      "source": [
        "#parallelize is used to create a rdd with the hardcoded values.\n",
        "#for creating a rdd with the external files we have mrthods like\n",
        "# spark.sparkContext.textFile(path) for creating rdd from textfile\n",
        "#spark.sparkContext.wholeTextFile(path) function returns a PairRDD with the key being the file path and value being file content\n",
        "\n",
        "\n",
        "#When we use parallelize() or textFile() or wholeTextFiles() methods of SparkContxt to initiate RDD, it automatically splits the data into partitions based on resource availability.\n",
        "#when you run it on a laptop it would create partitions as the same number of cores available on your system.\n",
        "\n",
        "#Set parallelize manually – We can also set a number of partitions manually,\n",
        "#all, we need is, to pass a number of partitions as the second parameter to these functions\n",
        "#for example  sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10)\n",
        "\n",
        "\n",
        "#getNumPartitions() – This a RDD function which returns a number of partitions our dataset split into.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YvOT50HFYl"
      },
      "source": [
        "# Create empty RDD using sparkContext.emptyRDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I88gWwBrHDtL",
        "outputId": "45347cad-27eb-4103-d30a-30623e7c0d33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "eRDD = spark.sparkContext.emptyRDD()\n",
        "eRDD.glom().collect()\n",
        "#creates empty RDD without partitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLBSA70KHhXL"
      },
      "source": [
        "# Creating empty RDD with partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KaAkzzKHRbB",
        "outputId": "447d3992-03a9-42d3-a872-985b26c75971"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], [], [], [], [], [], []]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "RDD = spark.sparkContext.parallelize([],10)\n",
        "RDD.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDo-s8xIuo8"
      },
      "source": [
        "Sometimes we may need to repartition the RDD, PySpark provides two ways to repartition\n",
        "# Repartition and Coalesce\n",
        "\n",
        "first using repartition() method which shuffles data from all nodes also called full shuffle\n",
        "\n",
        "second coalesce() method which shuffle data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.\n",
        "\n",
        "Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUFnzCLQHpjj",
        "outputId": "ff60952a-edb5-4aee-baad-5a3e58829b42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], [], [], [], [], [], []]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "RDD.coalesce(2)\n",
        "RDD.glom().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aAa61FnJQ66",
        "outputId": "a05ed019-37d6-4f4f-d45b-ca689c65e91a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[], [], [], [], [], [], [], [], [], []]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "RDD.repartition(4)\n",
        "RDD.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhoFeJRRJb4D"
      },
      "source": [
        "# PySpark RDD Operations\n",
        "\n",
        "RDD transformations: Transformations are lazy operations, instead of updating an RDD, these operations return another RDD.\n",
        "\n",
        "RDD actions :operations that trigger computation and return RDD values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "3fYUYqHYJWLo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD TRANSFORMATIONS"
      ],
      "metadata": {
        "id": "M5olbjkREdzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.map()\n",
        "As the name suggests, the .map() transformation maps a value to the elements of an RDD. The .map() transformation takes in an anonymous function and applies this function to each of the elements in the RDD. For example, If we want to add 10 to each of the elements present in RDD, the .map() transformation would come in handy. This operation saves time and goes with the DRY policy."
      ],
      "metadata": {
        "id": "PFZ7pQuzEiQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQUu2BebKGTe",
        "outputId": "e7f38018-0df0-4794-a3fc-a32a73194386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9, 25, 4], [40, 24, 16], [49, 11, 31], [35, 3, 35, 15, 35, 50]]\n",
            "After apply map function \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['9', '25', '4'],\n",
              " ['40', '24', '16'],\n",
              " ['49', '11', '31'],\n",
              " ['35', '3', '35', '15', '35', '50']]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#map() if we want to perform any transformation on each partition of RDD then we will be using map().\n",
        "# the function that we give to map will go through every element of RDD.\n",
        "#map() transformation is used the apply any complex operations like adding a column, updating a column e.t.c.\n",
        "#the output of map transformations would always have the same number of records as input.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(100)\n",
        "data = np.random.randint(1,51,size=15)\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data,4)\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "print(\"After apply map function \")\n",
        "rdd1 = rdd.map(lambda x : str(x))\n",
        "rdd1.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.filter()\n",
        "A .filter() transformation is an operation in PySpark for filtering elements from a PySpark RDD. The .filter() transformation takes in an anonymous function with a condition. Again, since it’s a transformation, it returns an RDD having elements that had passed the given condition."
      ],
      "metadata": {
        "id": "0zOJrwUoFjDN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d0Is2wFKoCu",
        "outputId": "9bf0b4ea-8ae1-447d-b86c-b22ff82c5650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12, 13, 14, 15]]\n",
            "[[2], [4, 6], [8], [10, 12, 14]]\n"
          ]
        }
      ],
      "source": [
        "#filter() it return the partition of elements which satisfyes the condition.\n",
        "\n",
        "d = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],4)\n",
        "print(d.glom().collect())\n",
        "\n",
        "D = d.filter(lambda x: x%2==0)\n",
        "print(D.glom().collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Union()\n",
        "\n",
        "The .union() transformation combines two RDDs and returns the union of the input two RDDs"
      ],
      "metadata": {
        "id": "P1uWvk0yGJol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jWhqPqYyP2P5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a226a514-bea6-4f4e-fb53-2add6a59580a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 15, 115)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "rr = rdd.union(RDD)\n",
        "RDD.count(),rdd.count(),rr.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.flatMap()\n",
        "The .flatMap() transformation peforms same as the .map() transformation except the fact that .flatMap() transformation return seperate values for each element from original RDD"
      ],
      "metadata": {
        "id": "PyOHlRC_Gdmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es8XYZiYQ3f8",
        "outputId": "b2dafd5d-9c69-4c5a-902d-46e7e668b910"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['SHIVA',\n",
              "  'shiva',\n",
              "  'SHNAKAR',\n",
              "  'shnakar',\n",
              "  'HARI',\n",
              "  'hari',\n",
              "  'KRISHNA',\n",
              "  'krishna',\n",
              "  'SAMBA',\n",
              "  'samba'],\n",
              " ['RAVI',\n",
              "  'ravi',\n",
              "  'BHARATH',\n",
              "  'bharath',\n",
              "  'AJAY',\n",
              "  'ajay',\n",
              "  'RAGAVA',\n",
              "  'ragava',\n",
              "  'MUNNA',\n",
              "  'munna']]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "names = ['shiva','shnakar','Hari','krishna','samba','ravi','bharath','ajay','ragava','munna']\n",
        "name_rdd = spark.sparkContext.parallelize(names,2)\n",
        "\n",
        "\n",
        "new_rdd = name_rdd.flatMap(lambda x:(x.upper(),x.lower()))\n",
        "new_rdd.glom().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RCPSQwJPHUlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RDD ACTIONS:\n",
        "Actions are a kind of operation which are applied on an RDD to produce a single value. These methods are applied on a resultant RDD and produces a non-RDD value, thus removing the laziness of the transformation of RDD."
      ],
      "metadata": {
        "id": "iU-IS3eu_eqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.collect()\n",
        "The .collect() action on an RDD returns a list of all the elements of the RDD. It’s a great asset for displaying all the contents of our RDD."
      ],
      "metadata": {
        "id": "H7ou7frAADTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "97Oa_hZC6dw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7c6841-4c89-4a36-d18a-207c4ef28c18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([521, 793, 836, 872, 856,  80, 945, 907, 351, 949, 867,  54, 579,\n",
              "       739, 527, 803, 753, 281, 656, 229, 876, 317, 571, 913, 508, 650,\n",
              "        94,  87, 387, 668, 877, 901, 416, 898, 142, 758, 724, 613,   5,\n",
              "       604, 956, 836, 136,  50, 432, 706, 318, 783, 696, 968, 764, 337,\n",
              "         3, 890, 618, 479, 404, 995,  64, 182, 284, 825, 239, 370, 927,\n",
              "       945, 304, 680, 878, 807, 173, 275, 193, 953, 931, 438, 715, 274,\n",
              "       585, 526, 619,  31,  18,  54,  69, 947, 489, 348, 476, 980, 694,\n",
              "       847,   1,  14, 186, 461, 363, 132, 583, 644])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(100)\n",
        "data = np.random.randint(1,1000,size=100)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = spark.sparkContext.parallelize(data,10)\n",
        "RDD.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq576_-nAVGp",
        "outputId": "f8fe1b69-ccef-432a-9874-6b4cd1734730"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[521, 793, 836, 872, 856, 80, 945, 907, 351, 949],\n",
              " [867, 54, 579, 739, 527, 803, 753, 281, 656, 229],\n",
              " [876, 317, 571, 913, 508, 650, 94, 87, 387, 668],\n",
              " [877, 901, 416, 898, 142, 758, 724, 613, 5, 604],\n",
              " [956, 836, 136, 50, 432, 706, 318, 783, 696, 968],\n",
              " [764, 337, 3, 890, 618, 479, 404, 995, 64, 182],\n",
              " [284, 825, 239, 370, 927, 945, 304, 680, 878, 807],\n",
              " [173, 275, 193, 953, 931, 438, 715, 274, 585, 526],\n",
              " [619, 31, 18, 54, 69, 947, 489, 348, 476, 980],\n",
              " [694, 847, 1, 14, 186, 461, 363, 132, 583, 644]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Count()\n",
        "The count() action on an RDD is an operation that returns the number of elements of our RDD."
      ],
      "metadata": {
        "id": "nladmlPRAxAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efUaQVz2Af4z",
        "outputId": "75500f0d-8095-429c-ab54-86c6f3577714"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.first()\n",
        "\n",
        "The .first() action on an RDD returns the first element from our RDD."
      ],
      "metadata": {
        "id": "LHK6C1l4BAps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.first()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB4jb3MsA2Qb",
        "outputId": "cb40f029-e5ff-4713-ec29-abf9fa2a6182"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "521"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.take(n)\n",
        "The .take(n) action on an RDD returns n number of elements from the RDD. The ‘n’ argument takes an integer which refers to the number of elements we want to extract from the RDD."
      ],
      "metadata": {
        "id": "yn-XmpPxBd1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYQuDPbpBVGJ",
        "outputId": "f7283c8c-dd11-402b-d209-ff3bae9e2c3e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[521, 793]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.reduce()\n",
        "The .reduce() Action takes two elements from the given RDD and operates. This operation is performed using an anonymous function or lambda.\n",
        "This is a costly opearation as it is iterates through every element of RDD."
      ],
      "metadata": {
        "id": "nMrG6lWpBtZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.reduce(lambda x,y:x+y)\n",
        "#Sum of all the elements"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfwpgfTJBj3r",
        "outputId": "596a0905-e77d-4981-b190-f874f20dfe74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53502"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.SaveAsTextFile()\n",
        "The .saveAsTextFile() Action is used to save the resultant RDD as a text file. We can also specify the path to which file needed to be saved."
      ],
      "metadata": {
        "id": "-YQ_tO6XC8er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD.coalesce(1)\n",
        "RDD.saveAsTextFile('sample_RDD1.txt')"
      ],
      "metadata": {
        "id": "SWSXBg_nCE1T"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Pair RDD Operations\n",
        "\n",
        "PySpark has a dedicated set of operations for Pair RDDs. Pair RDDs are a special kind of data structure in PySpark in the form of key-value pairs, and that’s how it got its name. Practically, the Pair RDDs are used more widely because of the reason that most of the real-world data is in the form of Key/Value pairs. The Pair RDDs use different terminology for key and value. The key is known as the identifier while the value is known as data."
      ],
      "metadata": {
        "id": "mu82a87mHZCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations in Pair RDDs"
      ],
      "metadata": {
        "id": "lAfM3P_PHdCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.reduceByKey()\n",
        "\n",
        "The .reduceByKey() transformation performs multiple parallel processes for each key in the data and combines the values for the same keys. It uses an anonymous function or lambda to perform the task. Since it’s a transformation, it returns an RDD as a result."
      ],
      "metadata": {
        "id": "MGN7qWPaHyKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks= [('shiva',100),('rohit',264),('virat',89),('virat',109),('shiva',400),('rohit',100)]\n",
        "\n",
        "marks_rdd = spark.sparkContext.parallelize(marks)\n",
        "\n",
        "print(marks_rdd.glom().collect())\n",
        "\n",
        "\n",
        "new_marks_rdd = marks_rdd.reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "new_marks_rdd.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCo_NuHZHYrf",
        "outputId": "be4980b4-f726-4909-bfaf-7c2f9b8f5c8f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('shiva', 100), ('rohit', 264), ('virat', 89)], [('virat', 109), ('shiva', 400), ('rohit', 100)]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('shiva', 500), ('virat', 198)], [('rohit', 364)]]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.sortByKey()\n",
        "\n",
        "The .sortByKey() transformation sorts the input data by keys from key-value pairs either in ascending or descending order. It returns a unique RDD as a result."
      ],
      "metadata": {
        "id": "lNQ6BqoII-6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_marks_rdd = marks_rdd.sortByKey()\n",
        "sorted_marks_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj06J0TTDfHi",
        "outputId": "6be057b0-2441-4ee6-cd68-d243e938347c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rohit', 264),\n",
              " ('rohit', 100),\n",
              " ('shiva', 100),\n",
              " ('shiva', 400),\n",
              " ('virat', 89),\n",
              " ('virat', 109)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.groupByKey()\n",
        "The .groupByKey() transformation groups all the values in the given data with the same key together."
      ],
      "metadata": {
        "id": "JhIuvyqZKMnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_marks_rdd = marks_rdd.groupByKey().collect()\n",
        "\n",
        "for key,value in grouped_marks_rdd:\n",
        "  print(key,list(value))\n",
        "\n",
        "\n",
        "#groupByKey() transformation on the marks_rdd.\n",
        "#Then we used the .collect() action to get the results and saved the results to grouped_marks_rdd.\n",
        "#Since grouped_marks_rdd is a dictionary item type, we applied the for loop on grouped_marks_rdd  to get a list of marks for each student in each line.\n",
        "#We also added list() to the values ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1ySxf20JRUZ",
        "outputId": "d564d503-b4ee-4be6-af24-54868e648302"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shiva [100, 400]\n",
            "virat [89, 109]\n",
            "rohit [264, 100]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actions on Pair RDDs"
      ],
      "metadata": {
        "id": "wL1MLaf4LvN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.countByKey()\n",
        "\n",
        "The .countByKey() option is used to count the number of values for each key in the given data. This action returns a dictionary and one can extract the keys and values by iterating over the extracted dictionary using loops. Since we are getting a dictionary as a result, we can also use the dictionary methods such as .keys(), .values() and .items()."
      ],
      "metadata": {
        "id": "hVxnRRfiL4eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r = marks_rdd.countByKey()\n",
        "for key,value in r.items():\n",
        "  print(key,value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWjK0R8qKvnh",
        "outputId": "4031fe61-5a7e-45ea-e764-223ab2c1e45d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shiva 2\n",
            "rohit 2\n",
            "virat 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD types\n",
        "\n",
        "PairRDDFunctions or PairRDD : Pair RDD is a key-value\n",
        "\n",
        "ShuffledRDD\n",
        "\n",
        "DoubleRDD\n",
        "\n",
        "SequenceFileRDD\n",
        "\n",
        "HadoopRDD\n",
        "\n",
        "ParallelCollectionRDD"
      ],
      "metadata": {
        "id": "-6ga7GfMTfbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#shuffeled operations\n",
        "Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS.\n",
        "\n",
        "PySpark Shuffle is an expensive operation since it involves the following\n",
        "\n",
        "Disk I/O\n",
        "\n",
        "Involves data serialization and deserialization\n",
        "\n",
        "Network I/O"
      ],
      "metadata": {
        "id": "ca_B4y4_ULKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Cache and persist\n",
        "\n",
        "PySpark Cache and Persist are optimization techniques to improve the performance of the RDD jobs that are iterative and interactive."
      ],
      "metadata": {
        "id": "IcoUOoWBUvQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though PySpark provides computation 100 x times faster than traditional Map Reduce jobs, If you have not designed the jobs to reuse the repeating computations you will see degrade in performance when you are dealing with billions or trillions of data. Hence, we need to look at the computations and use optimization techniques as one of the ways to improve performance."
      ],
      "metadata": {
        "id": "HsCksJmqU44O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using cache() and persist() methods, PySpark provides an optimization mechanism to store the intermediate computation of an RDD so they can be reused in subsequent actions.\n",
        "\n",
        "\n",
        "When you persist or cache an RDD, each worker node stores it’s partitioned data in memory or disk and reuses them in other actions on that RDD. And Spark’s persisted data on nodes are fault-tolerant meaning if any partition is lost, it will automatically be recomputed using the original transformations that created it."
      ],
      "metadata": {
        "id": "q8v5mZRUU68e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of Persisting RDD\n",
        "\n",
        "Cost efficient – PySpark computations are very expensive hence reusing the computations are used to save cost.\n",
        "\n",
        "Time efficient – Reusing the repeated computations saves lots of time.\n",
        "\n",
        "Execution time – Saves execution time of the job which allows us to perform more jobs on the same cluster."
      ],
      "metadata": {
        "id": "0a07Mf_FVJ0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD Cache\n",
        "PySpark RDD cache() method by default saves RDD computation to storage level `MEMORY_ONLY` meaning it will store the data in the JVM heap as unserialized objects.\n",
        "\n",
        "PySpark cache() method in RDD class internally calls persist() method which in turn uses sparkSession.sharedState.cacheManager.cacheQuery to cache the result set of RDD."
      ],
      "metadata": {
        "id": "pR8wZDIGVQbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD Persist\n",
        "PySpark persist() method is used to store the RDD to one of the storage levels MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2 and more.\n",
        "\n",
        "PySpark persist has two signature first signature doesn’t take any argument which by default saves it to <strong>MEMORY_ONLY</strong> storage level and the second signature which takes StorageLevel as an argument to store it to different storage levels."
      ],
      "metadata": {
        "id": "Hka3FRyGVcpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD Unpersist\n",
        "PySpark automatically monitors every persist() and cache() calls you make and it checks usage on each node and drops persisted data if not used or by using least-recently-used (LRU) algorithm. You can also manually remove using unpersist() method. unpersist() marks the RDD as non-persistent, and remove all blocks for it from memory and disk.\n",
        "\n"
      ],
      "metadata": {
        "id": "7zgKM4V6WDop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistence Storage Levels\n",
        "All different storage level PySpark supports are available at org.apache.spark.storage.StorageLevel class. Storage Level defines how and where to store the RDD.\n",
        "\n",
        "**MEMORY_ONLY **– This is the default behavior of the RDD cache() method and stores the RDD as deserialized objects to JVM memory. When there is no enough memory available it will not save to RDD of some partitions and these will be re-computed as and when required. This takes more storage but runs faster as it takes few CPU cycles to read from memory.\n",
        "\n",
        "**MEMORY_ONLY_SER –** This is the same as MEMORY_ONLY but the difference being it stores RDD as serialized objects to JVM memory. It takes lesser memory (space-efficient) then MEMORY_ONLY as it saves objects as serialized and takes an additional few more CPU cycles in order to deserialize.\n",
        "\n",
        "**MEMORY_ONLY_2 –** Same as MEMORY_ONLY storage level but replicate each partition to two cluster nodes.\n",
        "\n",
        "**MEMORY_ONLY_SER_2 –** Same as MEMORY_ONLY_SER storage level but replicate each partition to two cluster nodes.\n",
        "\n",
        "**MEMORY_AND_DISK –**In this Storage Level, The RDD will be stored in JVM memory as a deserialized objects. When required storage is greater than available memory, it stores some of the excess partitions in to disk and reads the data from disk when it required. It is slower as there is I/O involved.\n",
        "\n",
        "**MEMORY_AND_DISK_SER –** This is same as MEMORY_AND_DISK storage level difference being it serializes the RDD objects in memory and on disk when space not available.\n",
        "\n",
        "**MEMORY_AND_DISK_2 –** Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
        "\n",
        "**MEMORY_AND_DISK_SER_2 –** Same as MEMORY_AND_DISK_SER storage level but replicate each partition to two cluster nodes.\n",
        "\n",
        "**DISK_ONLY –** In this storage level, RDD is stored only on disk and the CPU computation time is high as I/O involved.\n",
        "\n",
        "**DISK_ONLY_2 –** Same as DISK_ONLY storage level but replicate each partition to two cluster nodes."
      ],
      "metadata": {
        "id": "DKR6vY40WI-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Shared Variables :\n",
        "\n",
        "Different types of PySpark Shared variables and how they are used in PySpark transformations.\n",
        "\n",
        "When PySpark executes transformation using map() or reduce() operations, It executes the transformations on a remote node by using the variables that are shipped with the tasks and these variables are not sent back to PySpark Driver hence there is no capability to reuse and sharing the variables across tasks. PySpark shared variables solve this problem using the below two techniques. PySpark provides two types of shared variables.\n",
        "\n",
        "**Broadcast variables** (read-only shared variable)\n",
        "\n",
        "**Accumulator variables** (updatable shared variables)\n",
        "\n",
        "**Broadcast read-only Variables**\n",
        "Broadcast variables are read-only shared variables that are cached and available on all nodes in a cluster in-order to access or use by the tasks. Instead of sending this data along with every task, PySpark distributes broadcast variables to the machine using efficient broadcast algorithms to reduce communication costs.\n",
        "\n",
        "One of the best use-case of PySpark RDD Broadcast is to use with lookup data for example zip code, state, country lookups e.t.c\n",
        "\n",
        "When you run a PySpark RDD job that has the Broadcast variables defined and used, PySpark does the following.\n",
        "\n",
        "PySpark breaks the job into stages that have distributed shuffling and actions are executed with in the stage.\n",
        "Later Stages are also broken into tasks\n",
        "PySpark broadcasts the common data (reusable) needed by tasks within each stage.\n",
        "The broadcasted data is cache in serialized format and deserialized before executing each task.\n",
        "The PySpark Broadcast is created using the broadcast(v) method of the SparkContext class. This method takes the argument v that you want to broadcast.\n",
        "\n",
        "\n",
        "broadcastVar = sc.broadcast([0, 1, 2, 3])\n",
        "\n",
        "broadcastVar.value\n",
        "\n",
        "Note that broadcast variables are not sent to executors with sc.broadcast(variable) call instead, they will be sent to executors when they are first used.\n",
        "\n",
        "Refer to PySpark RDD Broadcast shared variable for more detailed example.\n",
        "\n",
        "# Accumulators\n",
        "\n",
        "PySpark Accumulators are another type shared variable that are only “added” through an associative and commutative operation and are used to perform counters (Similar to Map-reduce counters) or sum operations.\n",
        "\n",
        "PySpark by default supports creating an accumulator of any numeric type and provides the capability to add custom accumulator types. Programmers can create following accumulators\n",
        "\n",
        "**named accumulators**\n",
        "\n",
        "**unnamed accumulators**\n",
        "\n",
        "When you create a named accumulator, you can see them on PySpark web UI under the “Accumulator” tab. On this tab, you will see two tables; the first table “accumulable” – consists of all named accumulator variables and their values. And on the second table “Tasks” – value for each accumulator modified by a task.\n",
        "\n",
        "Where as unnamed accumulators are not shows on PySpark web UI, For all practical purposes it is suggestable to use named accumulators.\n",
        "\n",
        "Accumulator variables are created using SparkContext.longAccumulator(v)"
      ],
      "metadata": {
        "id": "Q9FAcBYEWwwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark by default provides accumulator methods for long, double and collection types. All these methods are present in SparkContext class and return LongAccumulator, DoubleAccumulator, and CollectionAccumulator respectively.\n",
        "\n",
        "Long Accumulator\n",
        "\n",
        "Double Accumulator\n",
        "\n",
        "Collection Accumulator"
      ],
      "metadata": {
        "id": "SWmPFjnGXeG5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EElGyqrOMMYe"
      },
      "execution_count": 54,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}