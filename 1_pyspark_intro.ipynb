{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYpiHIHdxtcd2L7ReyOHVz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivashankarkammari/PySpark/blob/main/1_pyspark_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjk6gx1JcfhT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark is a\n",
        "\n",
        "**Distributed Execution Framework in**\n",
        "\n",
        "**Parallel processing style with**\n",
        "\n",
        "**in memeory compuatation**\n",
        "\n",
        "**used for large scale data processing**"
      ],
      "metadata": {
        "id": "fataHLbesaKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark is an Apache Spark library written in Python to run Python applications using Apache Spark capabilities.\n",
        "\n",
        "Apache Spark is an open-source unified analytics engine used for large-scale data processing, hereafter referred it as Spark.\n",
        "\n",
        "Spark is designed to be fast, flexible, and easy to use, making it a popular choice for processing large-scale data sets.\n",
        "\n",
        "Spark runs operations on billions and trillions of data on distributed clusters 100 times faster than traditional applications.\n",
        "\n",
        "Spark can run on single-node machines or multi-node machines(Cluster).\n",
        "\n",
        "It was created to address the limitations of MapReduce, by doing in-memory processing.\n",
        "\n",
        "Spark reuses data by using an in-memory cache to speed up machine learning algorithms that repeatedly call a function on the same dataset.\n",
        "\n",
        " This lowers the latency making Spark multiple times faster than MapReduce, especially when doing machine learning, and interactive analytics.  \n",
        "\n",
        " Apache Spark can also process real-time streaming.\n",
        "\n",
        " *Apache Spark was started by Matei Zaharia at UC Berkeley's AMPLab in 2009.*\n",
        "\n"
      ],
      "metadata": {
        "id": "lsyuXfcH5_lA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The following are the main features of Spark. All these features are applicable to PySpark as well.\n",
        "\n",
        "1.In-Memory Processing: Spark stores data in memory, which allows for faster data processing compared to traditional disk-based systems. This in-memory computation engine enables iterative and interactive data analysis.\n",
        "\n",
        "2.Resilient Distributed Datasets (RDDs): RDDs are Spark's fundamental data structure, which represents distributed collections of data that can be processed in parallel. RDDs are fault-tolerant and can be rebuilt from lineage information in case of data loss.\n",
        "\n",
        "3.Ease of Use: Spark provides APIs in several programming languages, including Scala, Java, Python, and R, making it accessible to a wide range of developers. It also supports SQL queries through Spark SQL and offers high-level libraries for machine learning (MLlib), graph processing (GraphX), and stream processing (Structured Streaming).\n",
        "\n",
        "4.Distributed Processing: Spark can distribute data processing across a cluster of machines, allowing for horizontal scalability. It can efficiently utilize resources in a cluster and automatically handle data partitioning and distribution.\n",
        "\n",
        "5.Built-in Libraries: Spark includes various libraries for common data processing tasks, such as batch processing, SQL querying, machine learning, graph processing, and streaming.\n",
        "\n",
        "6.Inbuild-optimization when using DataFramesInbuild-optimization when using DataFrames\n"
      ],
      "metadata": {
        "id": "MHvKruuU628w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of pyspark:\n",
        "\n",
        "1.Spark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficiently in a distributed fashion.\n",
        "\n",
        "2.Applications running on PySpark are 100x faster than traditional Pandas API.z\n",
        "\n",
        "3.You will get great benefits from using PySpark for data ingestion pipelines.\n",
        "Using PySpark we can process data from Hadoop HDFS, AWS S3, and many file systems.\n",
        "\n",
        "4.PySpark also is used to process real-time data using Streaming and Kafka.\n",
        "Using PySpark streaming you can also stream files from the file system and also stream from the socket.\n",
        "\n",
        "5.PySpark natively supports machine learning and graph libraries."
      ],
      "metadata": {
        "id": "ytO_-_BZ7pKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Architecture\n",
        "\n",
        "Apache Spark is a computing framework that processes data in parallel in a distributed environment with low latency.  Spark works on master node(driver) and slave(worker)node architecture on a Cluster. Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Below are the components of Spark.\n",
        "\n",
        "\n",
        "\n",
        "Cluster: A  cluster is a platform to launch the spark applications. A Spark cluster is a combination of a Driver Program, Cluster Manager, and Worker Nodes that work together to complete tasks. The SparkContext(Driver program) lets us coordinate processes across the cluster in order to execute the application. The SparkContext sends tasks to the Executors on the Worker Nodes to run.  The cluster can be a single node or multinode cluster based on the resource availability.\n",
        "\n",
        "Driver Program:  The Spark driver program is the one that creates the SparkContext object in the application. As soon as we submit the spark job, the driver program runs the main() method of your application and creates DAGs representing the data flow internally. Based on the DAG workflow, the driver requests the cluster manager to allocate the resources (workers) required for processing. Once the resources are allocated, the driver then using spark context sends the serialized result (code+data) to the workers to execute as Tasks and their result is captured.\n",
        "\n",
        "Cluster Manager:  Cluster Manager is the key component in the Spark architecture. It plays a very important role in terms of resource management. Cluster Manager makes sure the application runs efficiently and effectively by utilizing the resources of the cluster. There are a variety of cluster managers such as Hadoop YARN, Apache Mesos, and Standalone Scheduler.  Below are the key responsibilities of the Cluster Manager.\n",
        "\n",
        "Resource allocation: Cluster Manager plays a vital role in allocating the CPU cores, memory, and other resources to the application in a distributed environment.\n",
        "\n",
        "Task Scheduling:  Spark application is a set of tasks that has to be executed. Cluster Manager schedules the tasks to the resources available in the cluster based on the priorities.\n",
        "\n",
        "Fault tolerance: Cluster Manager also handles the failures. In case a worker node fails or a task fails, the Cluster Manager reschedules the task to any available nodes in the cluster, so that the task can be completed.\n",
        "\n",
        "Scalability:  Cluster Manager is designed to handle various sizes of Clusters. They can scale resource allocation and scheduling based on the availability of nodes on the Cluster.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Cluster Manager Types\n",
        "Spark supports below cluster managers:\n",
        "\n",
        "Standalone :A simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
        "\n",
        "Apache Mesos : Mesos is a Cluster manager that can also run Hadoop MapReduce and PySpark applications.\n",
        "\n",
        "Hadoop YARN :The resource manager in Hadoop 2. This is mostly used cluster manager.\n",
        "\n",
        "Kubernetes : An open-source system for automating deployment, scaling, and management of containerized applications.\n",
        "\n",
        "local : This is not really a cluster manager but still, I wanted to mention that as we use “local” in order to run Spark on your laptop.\n",
        "\n"
      ],
      "metadata": {
        "id": "jN1luupS8X1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark Ecosystem\n",
        "\n",
        "The Spark/PySpark ecosystem is a collection of tools, libraries, and components built around the framework.\n",
        "\n",
        "PySpark: PySpark is the core Python library for Apache Spark. It allows Python developers to interface with Spark's distributed data processing capabilities, making it easier to work with large datasets and perform distributed computing tasks using Python.\n",
        "\n",
        "PySpark SQL: Spark SQL is a component of Apache Spark that enables SQL-based querying of structured data, including data stored in Spark DataFrames and external data sources. PySpark provides a Python API for Spark SQL, allowing users to run SQL queries using Python.\n",
        "\n",
        "PySpark MLlib: Spark MLlib is Spark's machine learning library. It provides a wide range of machine learning algorithms and tools for building, training, and evaluating machine learning models. PySpark offers Python bindings for MLlib, making it accessible to Python developers.\n",
        "\n",
        "PySpark Streaming: Spark Streaming is a real-time data processing component of Spark that allows you to process and analyze data streams in real time. PySpark offers support for Spark Streaming, making it possible to build real-time data processing applications using Python.\n",
        "\n",
        "PySpark GraphX: GraphX is a graph processing library for Spark, and PySpark provides Python bindings for GraphX. This allows Python developers to work with graph data structures and perform graph analytics using Spark.\n",
        "\n",
        "PySpark Extensions: The PySpark ecosystem also includes various extensions and third-party libraries that enhance its capabilities. For example, libraries like Koalas provide a Pandas-like API for working with Spark DataFrames, making it easier for Pandas users to transition to PySpark.\n",
        "\n"
      ],
      "metadata": {
        "id": "V204U1rM9Eqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases and Applications of PySpark\n",
        "\n",
        "1.Data ETL (Extract, Transform, Load): PySpark is often used for data preprocessing tasks, including data extraction from various sources (databases, logs, CSV files), data transformation, and loading it into data warehouses or data lakes. It can handle large volumes of data in batch processing.\n",
        "\n",
        "2.Data Cleaning and Wrangling: PySpark's DataFrame APIsimplifies data cleaning and manipulation tasks. It helps in dealing with missing values, filtering, aggregating, and reshaping data to prepare it for analysis.\n",
        "\n",
        "3.Big Data Analytics: PySpark enables organizations to analyze large datasets efficiently. It supports various analytics tasks such as descriptive statistics, data summarization, and exploratory data analysis.\n",
        "\n",
        "4.Machine Learning: PySpark's MLlib library provides tools for building and deploying machine learning models at scale. It supports classification, regression, clustering, and recommendation tasks, making it suitable for predictive analytics applications. etc"
      ],
      "metadata": {
        "id": "d8ZX97h09rrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparkSession\n",
        "\n",
        "It is an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. It’s object spark is default available in pyspark-shell and it can be created programmatically using SparkSession."
      ],
      "metadata": {
        "id": "-9uHnJK8Achz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `SparkContext\n",
        "\n",
        "spark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variables. In this article, you will learn how to create PySpark SparkContext with examples. Note that you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method`.\n",
        "\n",
        "The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs, and know what resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application"
      ],
      "metadata": {
        "id": "EtfOeyOmA_80"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0GoSEQI61Za"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}