{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-jeg0ON9JVc",
        "outputId": "fe40bd97-81c2-45f4-b060-805dfb7f41dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession.builder.appName(\"DF\").getOrCreate()"
      ],
      "metadata": {
        "id": "uCtIs3MPSiTW"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Empty RDD with sparkContext.emptyRDD()\n"
      ],
      "metadata": {
        "id": "aqwknvKiafwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = spark.sparkContext.emptyRDD()\n",
        "df.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ63vINWTDA5",
        "outputId": "e9a5a176-c62b-4329-ec8d-b3c576889a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can also create empty RDD with\n",
        "\n",
        "emptyRDD = spark.sparkContext.parallelize([])\n",
        "emptyRDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWt99X0Ra0th",
        "outputId": "5d608bd0-243b-4dbb-8ac9-4873b6bb112d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating empty DF with req. schema"
      ],
      "metadata": {
        "id": "XuXJCnTDbOQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# suppose we need empty df for name,salary,designation,age,\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType().add('name',StringType(),False)\\\n",
        "                      .add('salary',FloatType(),False)\\\n",
        "                      .add('designation',StringType(),False)\\\n",
        "                      .add('age',IntegerType(),False)\n",
        "\n",
        "df = spark.createDataFrame(data =emptyRDD ,schema=schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_VjelxqbG9b",
        "outputId": "4fd26c20-fcfb-4d17-a185-3a4ed7168667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- salary: float (nullable = false)\n",
            " |-- designation: string (nullable = false)\n",
            " |-- age: integer (nullable = false)\n",
            "\n",
            "+----+------+-----------+---+\n",
            "|name|salary|designation|age|\n",
            "+----+------+-----------+---+\n",
            "+----+------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Empty RDD to empty_DF"
      ],
      "metadata": {
        "id": "eIozK9SldIr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = emptyRDD.toDF(schema)\n",
        "df1.printSchema()\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cWxV9nRcmjr",
        "outputId": "20281964-03a5-4aec-e850-1fcc0db73066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = false)\n",
            " |-- salary: float (nullable = false)\n",
            " |-- designation: string (nullable = false)\n",
            " |-- age: integer (nullable = false)\n",
            "\n",
            "+----+------+-----------+---+\n",
            "|name|salary|designation|age|\n",
            "+----+------+-----------+---+\n",
            "+----+------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.createDataFrame([],schema)\n",
        "df2.show(\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEK5f1R1dQ6L",
        "outputId": "0aed29a0-2c55-4dc9-d362-39f5ac13224f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-----------+---+\n",
            "|name|salary|designation|age|\n",
            "+----+------+-----------+---+\n",
            "+----+------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can convert rdd to DF as rdd.toDF()\n",
        "\n",
        "we can convert DF to pandas Df as df.toPandas()"
      ],
      "metadata": {
        "id": "n9yN6t83tWAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame([(12,'shiva'),(23,'shankar')],schema='id integer,name string')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efchrRM9eDFU",
        "outputId": "2fda0a35-b39f-4ab7-8da6-b5c8137f183c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "| 12|  shiva|\n",
            "| 23|shankar|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf= df.toPandas()\n",
        "print(pdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0bvVPZLwwPc",
        "outputId": "91a20fae-3d0f-4f6e-d285-0f9b568836dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id     name\n",
            "0  12    shiva\n",
            "1  23  shankar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column Functions:"
      ],
      "metadata": {
        "id": "kcMjapEf0gzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLUMN FUNCTION\tFUNCTION DESCRIPTION\n",
        "\n",
        "**alias(*alias, **kwargs) :** alias to the column or expressions\n",
        "\n",
        " **name(*alias, **kwargs):**\treturns same as alias().\n",
        "\n",
        "**asc()**:Returns ascending order of the column.\n",
        "\n",
        "**asc_nulls_first():**Returns null values first then non-null values.\n",
        "\n",
        "**asc_nulls_last() –** Returns null values after non-null values.\n",
        "\n",
        "**astype(dataType)**Used to cast the data type to another type.\n",
        "\n",
        "**cast(dataType)**\treturns same as cast().\n",
        "\n",
        "**between(lowerBound, upperBound)**\tChecks if the columns values are between lower and upper bound. Returns boolean value.\n",
        "\n",
        "**bitwiseAND(other)** Compute bitwise AND, OR & XOR of this expression with\n",
        "\n",
        "**bitwiseOR(other) **   another expression respectively.\n",
        "\n",
        "**bitwiseXOR(other)\t**\n",
        "\n",
        "**contains(other)**\tCheck if String contains in another string.\n",
        "\n",
        "**desc()** Returns descending order of the column.\n",
        "\n",
        "**desc_nulls_first()** -null values appear before non-null values.\n",
        "\n",
        "**desc_nulls_last() –** null values appear after non-null values.\n",
        "\n",
        "**startswith(other)**String starts with. Returns boolean expression\n",
        "\n",
        "**endswith(other)\t**String ends with. Returns boolean expression\n",
        "\n",
        "**eqNullSafe(other)**\tEquality test that is safe for null values.\n",
        "\n",
        "**getField(name)**\tReturns a field by name in a StructField and by key in Map.\n",
        "\n",
        "**getItem(key)**\tReturns a values from Map/Key at the provided position.\n",
        "\n",
        "**isNotNull()**Returns True if the current expression is NOT null.\n",
        "\n",
        "**isNull() –** Returns True if the current expression is null.\n",
        "\n",
        "**isin(*cols)**A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\n",
        "\n",
        "**like(other)**Similar to SQL like expression.\n",
        "\n",
        "**rlike(other)**\tSimilar to SQL RLIKE expression (LIKE with Regex)\n",
        ".\n",
        "**over(window)**\tUsed with window column\n",
        "\n",
        "**substr(startPos, length)**\tReturn a Column which is a substring of the column.\n",
        "\n",
        "**when(condition, value)\n",
        "otherwise(value)** Similar to SQL CASE WHEN, Executes a list of conditions and returns one of multiple possible result expressions.\n",
        "\n",
        "**dropFields(*fieldNames)**\tUsed to drops fields in StructType by name.\n",
        "\n",
        "**withField(fieldName, col)**\tAn expression that adds/replaces a field in StructType by name."
      ],
      "metadata": {
        "id": "1RcfeV-M05yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#alias()\n",
        "\n",
        "data=[(\"James\",\"Bond\",\"100\",None),\n",
        "      (\"Ann\",\"Varsa\",\"200\",'F'),\n",
        "      (\"Tom Cruise\",\"XXX\",\"400\",''),\n",
        "      (\"Tom Brand\",None,\"400\",'M')]\n",
        "columns=[\"fname\",\"lname\",\"id\",\"gender\"]\n",
        "df=spark.createDataFrame(data,columns)\n",
        "\n",
        "df.select(df.fname.alias('first_name'),df.lname.alias('last_name')\n",
        "   ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dtt4x2Lx2mt",
        "outputId": "05236e78-ad5b-4604-84a2-16d5bbd06c1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|first_name|last_name|\n",
            "+----------+---------+\n",
            "|     James|     Bond|\n",
            "|       Ann|    Varsa|\n",
            "|Tom Cruise|      XXX|\n",
            "| Tom Brand|     NULL|\n",
            "+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,asc\n",
        "\n",
        "#Note: when do we use asc(), desc() ? while sorting !\n",
        "\n",
        "data = [(100,'ram'),(23,'sham'),(1,'bheem'),(12,'krish')]\n",
        "df = spark.createDataFrame(data,schema = 'id integer,name string')\n",
        "\n",
        "df.sort(df.id.asc()).show(),df.sort(df.id.desc()).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af7lTs8e5L0U",
        "outputId": "4eb69ee3-47a7-44ec-8c72-6584bc33b728"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1|bheem|\n",
            "| 12|krish|\n",
            "| 23| sham|\n",
            "|100|  ram|\n",
            "+---+-----+\n",
            "\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|100|  ram|\n",
            "| 23| sham|\n",
            "| 12|krish|\n",
            "|  1|bheem|\n",
            "+---+-----+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "9faJzq5H7nwj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One of the simplest ways to create a Column class object is by using PySpark lit() SQL function, this takes a literal value and returns a Column object.**"
      ],
      "metadata": {
        "id": "LfRfLV-9W5Ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colObj = lit('shiva')\n",
        "print(colObj)"
      ],
      "metadata": {
        "id": "mJ14LyNC_jST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89fadee-c11f-41cf-b5ac-aa856d0825de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'shiva'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(\"James\",23),(\"Ann\",40)]\n",
        "df=spark.createDataFrame(data).toDF(\"name.fname\",\"gender\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZBytNhwXBJ2",
        "outputId": "8d4e4954-fd04-4177-fa26-148c9155ff65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name.fname: string (nullable = true)\n",
            " |-- gender: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Different ways of Accessing the df columns\n",
        "\n",
        "# df.select(df.gender).show()\n",
        "# df.select(col('gender')).show() # this using pysqpark.sql.functions col()\n",
        "# df.select(df['gender']).show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKoW9upaXgbn",
        "outputId": "e8bfb161-4dae-4d00-875b-4d593b834621"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|gender|\n",
            "+------+\n",
            "|    23|\n",
            "|    40|\n",
            "+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
        "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
        "# df.show()\n",
        "\n",
        "#Arthmetic operations\n",
        "# df.select(df.col1 + df.col2).show()\n",
        "# df.select(df.col1 - df.col2).show()\n",
        "# df.select(df.col1 * df.col2).show()\n",
        "# df.select(df.col1 / df.col2).show()\n",
        "# df.select(df.col1 % df.col2).show()\n",
        "\n",
        "# Relational Operators\n",
        "# df.select((df.col2 > df.col3).alias(\"Greater_than\")).show()\n",
        "# df.select((df.col2 < df.col3).alias('Less_than')).show()\n",
        "# df.select((df.col2 == df.col3).alias(\"Equal\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nRDN1BBXpiu",
        "outputId": "98ce2fe8-71cb-412a-88be-031ff7e73ca8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "| 100|   2|   1|\n",
            "| 200|   3|   4|\n",
            "| 300|   4|   4|\n",
            "+----+----+----+\n",
            "\n",
            "+-----+\n",
            "|Equal|\n",
            "+-----+\n",
            "|false|\n",
            "|false|\n",
            "| true|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select()\n",
        "\n",
        "select is used to select columns of the DF.\n",
        "\n",
        "**Select Single & Multiple Columns :**\n",
        "\n",
        "df.select(\"firstname\",\"lastname\").show()\n",
        "\n",
        "df.select(df.firstname,df.lastname).show()\n",
        "\n",
        "df.select(df[\"firstname\"],df[\"lastname\"]).show()\n",
        "\n",
        "#By using col() function\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df.select(col(\"firstname\"),col(\"lastname\")).show()\n",
        "\n",
        "**Selecting all columns**\n",
        "\n",
        "df.select(\"*\").show()\n",
        "\n",
        "**Select Columns by Index**\n",
        "\n",
        "df.select(df.columns[:3]).show(3)\n",
        "\n",
        "\n",
        "\n",
        "**column functions that works along select are**\n",
        "\n",
        "1.cast(dtype),astype(dtype)\n",
        "\n",
        "2.alias(),name()\n",
        "\n",
        "3.substr()\n",
        "\n",
        "4.when() otherwise()\n",
        "\n",
        "5.getField() & getItem()\n",
        "\n"
      ],
      "metadata": {
        "id": "1az3WYTAiaJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dd = spark.read.csv('/content/driver-data (2).csv',header=True,inferSchema=True)"
      ],
      "metadata": {
        "id": "o68iEZrjYdVR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd.show(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTKXyvNMjuVO",
        "outputId": "2b994bfd-1744-459d-aea2-844171c945ec"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+--------------------+\n",
            "|        id|mean_dist_day|mean_over_speed_perc|\n",
            "+----------+-------------+--------------------+\n",
            "|3423311935|        71.24|                  28|\n",
            "|3423313212|        52.53|                  25|\n",
            "|3423313724|        64.54|                  27|\n",
            "|3423311373|        55.69|                  22|\n",
            "|3423310999|        54.58|                  25|\n",
            "|3423313857|        41.91|                  10|\n",
            "|3423312432|        58.64|                  20|\n",
            "|3423311434|        52.02|                   8|\n",
            "|3423311328|        31.25|                  34|\n",
            "|3423312488|        44.31|                  19|\n",
            "|3423311254|        49.35|                  40|\n",
            "|3423312943|        58.07|                  45|\n",
            "+----------+-------------+--------------------+\n",
            "only showing top 12 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#converting mean_over_speed_perc to a floattype\n",
        "\n",
        "dd_mean_over_speed = dd.select(dd.mean_over_speed_perc.cast('float'))\n",
        "dd_mean_over_speed.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja1MfLayjwnj",
        "outputId": "3f25c486-65aa-43ab-d860-de92a7bc0acb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|mean_over_speed_perc|\n",
            "+--------------------+\n",
            "|                28.0|\n",
            "|                25.0|\n",
            "|                27.0|\n",
            "|                22.0|\n",
            "|                25.0|\n",
            "|                10.0|\n",
            "|                20.0|\n",
            "|                 8.0|\n",
            "|                34.0|\n",
            "|                19.0|\n",
            "|                40.0|\n",
            "|                45.0|\n",
            "|                22.0|\n",
            "|                19.0|\n",
            "|                43.0|\n",
            "|                32.0|\n",
            "|                35.0|\n",
            "|                27.0|\n",
            "|                26.0|\n",
            "|                30.0|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using alias\n",
        "\n",
        "dd_mean_over_speed.select(dd_mean_over_speed.mean_over_speed_perc.alias(\"Mean_overSpeed\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPCaddgPkLzt",
        "outputId": "58c5887a-6072-4ec9-a3bf-8f338f1d9642"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|Mean_overSpeed|\n",
            "+--------------+\n",
            "|          28.0|\n",
            "|          25.0|\n",
            "|          27.0|\n",
            "|          22.0|\n",
            "|          25.0|\n",
            "|          10.0|\n",
            "|          20.0|\n",
            "|           8.0|\n",
            "|          34.0|\n",
            "|          19.0|\n",
            "|          40.0|\n",
            "|          45.0|\n",
            "|          22.0|\n",
            "|          19.0|\n",
            "|          43.0|\n",
            "|          32.0|\n",
            "|          35.0|\n",
            "|          27.0|\n",
            "|          26.0|\n",
            "|          30.0|\n",
            "+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd = spark.read.csv('/content/police.csv',header=True)"
      ],
      "metadata": {
        "id": "LqQgC9zQk7Of"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoUdc3UNmoHs",
        "outputId": "6054b2d5-6419-4db7-d9e1-ed57e38b3316"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+-----------+-------------+--------------+----------+-----------+--------------------+---------+----------------+-----------+-------------+-----------+-------------+------------------+\n",
            "| stop_date|stop_time|county_name|driver_gender|driver_age_raw|driver_age|driver_race|       violation_raw|violation|search_conducted|search_type| stop_outcome|is_arrested|stop_duration|drugs_related_stop|\n",
            "+----------+---------+-----------+-------------+--------------+----------+-----------+--------------------+---------+----------------+-----------+-------------+-----------+-------------+------------------+\n",
            "|2005-01-02|    01:55|       NULL|            M|        1985.0|      20.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-01-18|    08:15|       NULL|            M|        1965.0|      40.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-01-23|    23:15|       NULL|            M|        1972.0|      33.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-02-20|    17:15|       NULL|            M|        1986.0|      19.0|      White|    Call for Service|    Other|           False|       NULL|Arrest Driver|       True|    16-30 Min|             False|\n",
            "|2005-03-14|    10:00|       NULL|            F|        1984.0|      21.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-03-23|    09:45|       NULL|            M|        1982.0|      23.0|      Black|Equipment/Inspect...|Equipment|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-04-01|    17:30|       NULL|            M|        1969.0|      36.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-06-06|    13:20|       NULL|            F|        1986.0|      19.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-13|    10:15|       NULL|            M|        1970.0|      35.0|      Black|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-13|    15:45|       NULL|            M|        1970.0|      35.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-13|    16:20|       NULL|            M|        1979.0|      26.0|      Asian|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-13|    19:00|       NULL|            F|        1966.0|      39.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-14|    19:55|       NULL|            M|        1979.0|      26.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-18|    19:30|       NULL|            F|        1984.0|      21.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-18|    19:45|       NULL|            M|        1969.0|      36.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-19|    00:30|       NULL|            M|        1982.0|      23.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-19|    00:30|       NULL|            M|        1982.0|      23.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-19|    23:30|       NULL|            M|        1979.0|      26.0|      White|Equipment/Inspect...|Equipment|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-20|    00:05|       NULL|            M|        1955.0|      50.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "|2005-07-24|    20:10|       NULL|            F|        1958.0|      47.0|      White|            Speeding| Speeding|           False|       NULL|     Citation|      False|     0-15 Min|             False|\n",
            "+----------+---------+-----------+-------------+--------------+----------+-----------+--------------------+---------+----------------+-----------+-------------+-----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#substr\n",
        "pd.select(pd.stop_outcome.substr(0,6)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwvEJNJAnMeX",
        "outputId": "35120e03-44b0-4277-bd29-dbe171bd7a1c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+\n",
            "|substring(stop_outcome, 0, 6)|\n",
            "+-----------------------------+\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Arrest|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "|                       Citati|\n",
            "+-----------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#when() and otherwise()\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "pd.select(pd.is_arrested,(when(pd.is_arrested == 'True',\"Arrested!\").otherwise(\"Not_Arrested :)\")).alias('Result')).show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7kpY725pMLo",
        "outputId": "b2dd82af-b306-4305-fa6e-6582809144d3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+\n",
            "|is_arrested|         Result|\n",
            "+-----------+---------------+\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|       True|      Arrested!|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "|      False|Not_Arrested :)|\n",
            "+-----------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getField() and getItem()\n",
        "\n",
        "#Create DataFrame with struct, array & map\n",
        "from pyspark.sql.types import StructType,StructField,StringType,ArrayType,MapType\n",
        "data=[((\"James\",\"Bond\"),[\"Java\",\"C#\"],{'hair':'black','eye':'brown'}),\n",
        "      ((\"Ann\",\"Varsa\"),[\".NET\",\"Python\"],{'hair':'brown','eye':'black'}),\n",
        "      ((\"Tom Cruise\",\"\"),[\"Python\",\"Scala\"],{'hair':'red','eye':'grey'}),\n",
        "      ((\"Tom Brand\",None),[\"Perl\",\"Ruby\"],{'hair':'black','eye':'blue'})]\n",
        "\n",
        "schema = StructType([\n",
        "        StructField('name', StructType([\n",
        "            StructField('fname', StringType(), True),\n",
        "            StructField('lname', StringType(), True)])),\n",
        "        StructField('languages', ArrayType(StringType()),True),\n",
        "        StructField('properties', MapType(StringType(),StringType()),True)\n",
        "     ])\n",
        "\n",
        "# schema = StructType([\n",
        "#     StructField('name',StructType([\n",
        "#         StructField('fname',StringType()),\n",
        "#         StructField('lname',StringType())\n",
        "#     ]),\n",
        "#     StructField(\"languages\",ArrayType(StringType())),\n",
        "#    StructField(\"properties\",MapType(StringType(),StringType())))\n",
        "# ])\n",
        "\n",
        "\n",
        "df=spark.createDataFrame(data,schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvX83Mvmp2ik",
        "outputId": "147f1ee5-a20c-42f3-d8b6-90e4395611a5"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- fname: string (nullable = true)\n",
            " |    |-- lname: string (nullable = true)\n",
            " |-- languages: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+-----------------+---------------+-----------------------------+\n",
            "|name             |languages      |properties                   |\n",
            "+-----------------+---------------+-----------------------------+\n",
            "|{James, Bond}    |[Java, C#]     |{eye -> brown, hair -> black}|\n",
            "|{Ann, Varsa}     |[.NET, Python] |{eye -> black, hair -> brown}|\n",
            "|{Tom Cruise, }   |[Python, Scala]|{eye -> grey, hair -> red}   |\n",
            "|{Tom Brand, NULL}|[Perl, Ruby]   |{eye -> blue, hair -> black} |\n",
            "+-----------------+---------------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For mapTypes we use getFiled to access the elements\n",
        "df.select(df.properties.getField('hair')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnghwHPCtxMf",
        "outputId": "47b66a72-6de5-44ce-d63c-4686215d1f13"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+\n",
            "|properties[hair]|\n",
            "+----------------+\n",
            "|           black|\n",
            "|           brown|\n",
            "|             red|\n",
            "|           black|\n",
            "+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#getItem() to get the elements with indexing or either by label.\n",
        "#for arrayType,maType,nyType we can use this getItem(index OR label)\n",
        "\n",
        "# df.select(df.properties.getItem('eye')).show()\n",
        "df.select(df.languages.getItem(0\n",
        "                               )).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUS4NGa2urYC",
        "outputId": "5eb8c912-90fc-4239-f739-a6458295f372"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|languages[0]|\n",
            "+------------+\n",
            "|        Java|\n",
            "|        .NET|\n",
            "|      Python|\n",
            "|        Perl|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter\n",
        "\n",
        "Filter is just like where clause in SQL where we can apply some conditions.\n",
        "\n",
        "cloumn functions that works along filter are\n",
        "\n",
        "1.between(lowerBound, upperBound)\n",
        "\n",
        "2.rlike(other) and like(other)\n",
        "\n",
        "3.isin(*cols)\n",
        "\n",
        "4.isNull() & isNotNull()\n",
        "\n",
        "5.startswith(other) & endswith(other)"
      ],
      "metadata": {
        "id": "YJ_XvEVOv0-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#between(lb,hb)\n",
        "\n",
        "Ages21T45  = pd.select(pd.driver_age).filter(pd.driver_age.between(30,45).alias(\"AGE_b/w21&45\"))\n",
        "\n",
        "Ages21T45.sort(Ages21T45.driver_age.asc()).count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUu2m7x5vK52",
        "outputId": "7f2777a7-4b75-404e-ee06-840efce11302"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9931"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#contains\n",
        "\n",
        "df = spark.createDataFrame([(\"Hello World\",\"Hello Earth\"),(\"Hii Anna\",\"Hii MAMA\"),(\"Hello Kushi\",\"hi RAMj\"),(\"Shiva\",\"Shankar\")],schema='col1 string,col2 string')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7OBMNzWxSJQ",
        "outputId": "0fba4bcf-7ca4-402d-bf1c-234651471230"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "|       col1|       col2|\n",
            "+-----------+-----------+\n",
            "|Hello World|Hello Earth|\n",
            "|   Hii Anna|   Hii MAMA|\n",
            "|Hello Kushi|    hi RAMj|\n",
            "|      Shiva|    Shankar|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#conatins\n",
        "df.select(df.col2).filter(df.col1.contains('hi')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZc1570gy_es",
        "outputId": "dc5205a2-167a-464e-a3c3-d41d20757365"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|   col2|\n",
            "+-------+\n",
            "|hi RAMj|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#startswith and endswith\n",
        "\n",
        "df.select(df.col1).filter(df.col1.startswith(\"S\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vSGTwF3zc8c",
        "outputId": "d29b9517-4fdb-42b3-c52b-487745b1f5aa"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "| col1|\n",
            "+-----+\n",
            "|Shiva|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# df.show()\n",
        "\n",
        "# Default - displays 20 rows and\n",
        "# 20 charactes from column value\n",
        "df.show()\n",
        "\n",
        "#Display full column contents\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Display 2 rows and full column contents\n",
        "df.show(2,truncate=False)\n",
        "\n",
        "# Display 2 rows & column values 25 characters\n",
        "df.show(2,truncate=25)\n",
        "\n",
        "# Display DataFrame rows & columns vertically\n",
        "df.show(n=3,truncate=25,vertical=True)\n",
        "\n",
        "Syntax\n",
        "def show(self, n=20, truncate=True, vertical=False)"
      ],
      "metadata": {
        "id": "SCBeMuLc2SPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# withcolumn()\n",
        "\n",
        "withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more."
      ],
      "metadata": {
        "id": "3Wubmr749vXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change column Dtype**"
      ],
      "metadata": {
        "id": "GZEInTaZ967u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgjIHbq1Nm0",
        "outputId": "883ed529-3642-42b8-d3c9-de48bf4f5eac"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "df.withColumn(\"salary\",col('salary').cast('integer')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tceldRgV-OpT",
        "outputId": "5016e767-7383-49fe-edb8-1ac08b144321"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Column from an Existing**"
      ],
      "metadata": {
        "id": "4WAFovAiBxVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a Bonus col with salary col\n",
        "\n",
        "df.withColumn(\"Bonus\",col(\"salary\")+col(\"salary\")*0.6).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foW2wd_V-iM0",
        "outputId": "bbafbce9-9fb0-4e21-b799-9a103ba834b5"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary| Bonus|\n",
            "+---------+----------+--------+----------+------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|4800.0|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|6400.0|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|6400.0|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|6400.0|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|  -1.6|\n",
            "+---------+----------+--------+----------+------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add a New Column using withColumn()**"
      ],
      "metadata": {
        "id": "2FDI_2Z7CSMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df  = df.withColumn(\"Designation\",lit(when(col('salary')<=3000,\"Staff\").otherwise(\"Senior\")))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkP-X9nLCJ0A",
        "outputId": "b5daaae5-80b2-47b9-caf2-59215e1d6df0"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+------+-----------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|Designation|\n",
            "+---------+----------+--------+----------+------+------+-----------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|      Staff|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|     Senior|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|     Senior|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|     Senior|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|      Staff|\n",
            "+---------+----------+--------+----------+------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Renaming the column with withColumnRenamed()**"
      ],
      "metadata": {
        "id": "gKe7C4TPDf8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.withColumnRenamed(\"salary\",\"PayScale\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krllu8qgCs5K",
        "outputId": "6865c6de-51d4-4e23-fdb3-f86614cff509"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+----------+------+--------+-----------+\n",
            "|firstname|middlename|lastname|       dob|gender|PayScale|Designation|\n",
            "+---------+----------+--------+----------+------+--------+-----------+\n",
            "|    James|          |   Smith|1991-04-01|     M|    3000|      Staff|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|    4000|     Senior|\n",
            "|   Robert|          |Williams|1978-09-05|     M|    4000|     Senior|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|    4000|     Senior|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|      -1|      Staff|\n",
            "+---------+----------+--------+----------+------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# distinct()\n",
        "\n",
        "**Get Distinct Rows (By Comparing All Columns)**"
      ],
      "metadata": {
        "id": "5CM2AsL2EFQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600), \\\n",
        "    (\"Robert\", \"Sales\", 4100), \\\n",
        "    (\"Maria\", \"Finance\", 3000), \\\n",
        "    (\"James\", \"Sales\", 3000), \\\n",
        "    (\"Scott\", \"Finance\", 3300), \\\n",
        "    (\"Jen\", \"Finance\", 3900), \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000), \\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  ]\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_52pCEfEDOvE",
        "outputId": "95eac70d-ac15-466e-eee9-5a61ecc43877"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count(),df.distinct().count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYHJqC9yEbUi",
        "outputId": "b9ef457d-7fae-4b9e-98ed-add5489acd22"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dropduplicates([cols*])**--> return distinct records based on the unique col"
      ],
      "metadata": {
        "id": "Ux5XTnRHF_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates([\"salary\",\"department\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqR89xa7FCcP",
        "outputId": "e70c16d2-9e28-4caa-c0e3-dab100c1e0f9"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        Kumar| Marketing|  2000|\n",
            "|        Maria|   Finance|  3000|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|       Robert|     Sales|  4100|\n",
            "|      Michael|     Sales|  4600|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sort() / orderBy()\n",
        "\n",
        "\n",
        "PySpark DataFrame class provides sort() function to sort on one or more columns. By default, it sorts by ascending order."
      ],
      "metadata": {
        "id": "hPjUH4mwGi9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.sort(col('salary').desc()).show()\n",
        "\n",
        "df.orderBy(col('salary').desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BfKpM1WFRG8",
        "outputId": "749bd7b7-0ddc-4d96-cb74-c91d6036a0cd"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|      Michael|     Sales|  4600|\n",
            "|         Saif|     Sales|  4100|\n",
            "|       Robert|     Sales|  4100|\n",
            "|          Jen|   Finance|  3900|\n",
            "|        Scott|   Finance|  3300|\n",
            "|        James|     Sales|  3000|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Maria|   Finance|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# groupBy()\n",
        "\n",
        "When we perform groupBy() on PySpark Dataframe, it returns GroupedData object which contains below aggregate functions.\n",
        "\n",
        "count() - Use groupBy() count() to return the number of rows for each group.\n",
        "\n",
        "mean() - Returns the mean of values for each group.\n",
        "\n",
        "max() - Returns the maximum of values for each group.\n",
        "\n",
        "min() - Returns the minimum of values for each group.\n",
        "\n",
        "sum() - Returns the total for values for each group.\n",
        "\n",
        "avg() - Returns the average for values for each group.\n",
        "\n",
        "agg() - Using groupBy() agg() function, we can calculate more than one aggregate at a time.\n",
        "\n",
        "pivot() - This function is used to Pivot the DataFrame."
      ],
      "metadata": {
        "id": "X77_4NZIG_L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"department\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxzpLptOGrqw",
        "outputId": "24120bcb-0b21-40ef-a6be-1c7cc6b51221"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    5|\n",
            "|   Finance|    3|\n",
            "| Marketing|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
        "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
        "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
        "  ]\n",
        "\n",
        "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kCbC0uqHaIv",
        "outputId": "6f75a229-2fdb-41c5-aecf-1790e89e2e7b"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
            "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find min,max,mean,sum of salary for the group of department\n",
        "\n",
        "df.groupBy('department').min(\"salary\").show(),\n",
        "df.groupBy('department').max(\"salary\").show(),\n",
        "df.groupBy('department').sum(\"salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxKJqGENI2mh",
        "outputId": "c7c72302-41bc-4e6c-f7d4-05becd9e8913"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|department|min(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|      81000|\n",
            "|   Finance|      79000|\n",
            "| Marketing|      80000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|department|max(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|      90000|\n",
            "|   Finance|      99000|\n",
            "| Marketing|      91000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|department|sum(salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     257000|\n",
            "|   Finance|     351000|\n",
            "| Marketing|     171000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using multiple columns\n",
        "\n",
        "df.groupBy(\"department\",\"state\").sum(\"salary\",\"bonus\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niaR61HcJTwB",
        "outputId": "da57e02f-5216-457b-c220-69dcf575511d"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----------+----------+\n",
            "|department|state|sum(salary)|sum(bonus)|\n",
            "+----------+-----+-----------+----------+\n",
            "|     Sales|   CA|      81000|     23000|\n",
            "|   Finance|   CA|     189000|     47000|\n",
            "|     Sales|   NY|     176000|     30000|\n",
            "|   Finance|   NY|     162000|     34000|\n",
            "| Marketing|   NY|      91000|     21000|\n",
            "| Marketing|   CA|      80000|     18000|\n",
            "+----------+-----+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupBy using agg()\n",
        "\n",
        "\n",
        "df.groupBy(\"department\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
        "         max(\"bonus\").alias(\"max_bonus\") \\\n",
        "     ) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozNrsDKMK3Em",
        "outputId": "dd4204a5-83a6-464f-e433-2eb6b099960e"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------------+---------+---------+\n",
            "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
            "|Finance   |351000    |87750.0          |81000    |24000    |\n",
            "|Marketing |171000    |85500.0          |39000    |21000    |\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#groupBy() with filter()\n",
        "\n",
        "df.groupBy(\"department\") \\\n",
        "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
        "      avg(\"salary\").alias(\"avg_salary\"), \\\n",
        "      sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
        "      max(\"bonus\").alias(\"max_bonus\")) \\\n",
        "    .where(col(\"sum_bonus\") >= 50000) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lC_XiQeLiAy",
        "outputId": "43292966-4e35-45a4-cf4a-9f9fa603c017"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------------+---------+---------+\n",
            "|department|sum_salary|avg_salary       |sum_bonus|max_bonus|\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "|Sales     |257000    |85666.66666666667|53000    |23000    |\n",
            "|Finance   |351000    |87750.0          |81000    |24000    |\n",
            "+----------+----------+-----------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# join()\n",
        "\n",
        "join(self, other, on=None, how=None)\n",
        "\n",
        "join() operation takes parameters as below and returns DataFrame.\n",
        "\n",
        "param other: Right side of the join\n",
        "\n",
        "param on: a string for the join column name\n",
        "\n",
        "param how: default inner. Must be one of inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti."
      ],
      "metadata": {
        "id": "oh-_QBkXNNgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset\n",
        "\n",
        "emp = [(1,\"Smith\",1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)\n",
        "\n",
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "deptDF.printSchema()\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15jjqtQoMioP",
        "outputId": "2d53e260-2bb5-41e1-87fe-be7f7b60be2a"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |1              |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n",
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner Join**\n"
      ],
      "metadata": {
        "id": "vi2ARm7mPu7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"inner\").show()\n",
        "\n",
        "#Inner join is the default join in PySpark and it's mostly used.\n",
        "# This joins two datasets on key columns, where keys don't match the rows get dropped from both datasets (emp & dept).\n",
        "\n",
        "# When we apply Inner join on our datasets,\n",
        "# It drops \"emp_dept_id\" 50 from \"emp\" and \"dept_id\" 30 from \"dept\" datasets.\n",
        "# Below is the result of the above Join expression."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAEN1zaWN3VT",
        "outputId": "ded2b853-1a16-44cd-ec76-25a5cd2fa374"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|   Smith|              1|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
            "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outer Join**"
      ],
      "metadata": {
        "id": "cEYI-x8oPrvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Outer a.k.a full, fullouter join returns all rows from both datasets,\n",
        "  # where join expression doesn't match it returns null on respective record columns.\n",
        "\n",
        "  # From our \"emp\" dataset's \"emp_dept_id\" with value 50 doesn't have a record on \"dept\"\n",
        "  # hence dept columns have null and \"dept_id\" 30 doesn't have a record in \"emp\" hence you see null's on emp columns.\n",
        "  # Below is the result of the above Join expression.\n",
        "\n",
        "  empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"outer\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psp_Mv0dOGw3",
        "outputId": "4ef0fc81-6800-4272-86a1-326a31e0b803"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|     1|   Smith|              1|       2018|         10|     M|  3000|  Finance|     10|\n",
            "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
            "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
            "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
            "|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n",
            "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
            "|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Join**\n",
        "\n",
        "Left a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn't match, it assigns null for that record and drops records from right where match not found.\n",
        "\n",
        "From our dataset, \"emp_dept_id\" 5o doesn't have a record on \"dept\" dataset hence, this record contains null on \"dept\" columns (dept_name & dept_id). and \"dept_id\" 30 from \"dept\" dataset dropped from the results. Below is the result of the above Join expression."
      ],
      "metadata": {
        "id": "-AFE1k8bP3Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\n",
        "  # empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\")\n",
        "  #   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMZe-Eq9PnSX",
        "outputId": "f1942f0c-e329-4e8d-ad7b-e46e3d8ca328"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |1              |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right Join**\n",
        "\n",
        "Right a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesn't match, it assigns null for that record and drops records from left where match not found.\n",
        "\n",
        "From our example, the right dataset \"dept_id\" 30 doesn't have it on the left dataset \"emp\" hence, this record contains null on \"emp\" columns. and \"emp_dept_id\" 50 dropped as a match not found on left. Below is the result of the above Join expression.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRPZnQomRKhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "# empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
        "#    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMgQfde8Q328",
        "outputId": "5ba8d6bf-6846-45f7-f37c-327a72b8eb12"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |1              |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left semijoin**\n",
        "l\n",
        "leftsemi join is similar to inner join difference being leftsemi join returns all columns from the left dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n",
        "\n",
        "The same result can be achieved using select on the result of the inner join however, using this join would be efficient."
      ],
      "metadata": {
        "id": "Gb5trYI9SJ-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Anti**\n",
        "\n",
        "leftanti join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset for non-matched records."
      ],
      "metadata": {
        "id": "EvM2rzQJSR2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Union() and UnionAll()\n",
        "\n",
        "Dataframe union() - union() method of the DataFrame is used to merge two DataFrame's of the same structure/schema. If schemas are not the same it returns an error.\n",
        "\n",
        "DataFrame unionAll() - unionAll() is deprecated since Spark \"2.0.0\" version and replaced with union()."
      ],
      "metadata": {
        "id": "S_TmttpDTFr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "\n",
        "\n",
        "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
        "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
        "  ]\n",
        "\n",
        "columns= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTyO3z4rRYpE",
        "outputId": "6e47f17f-b63a-4d32-9a90-c7a104b3eab9"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
            "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData2 = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
        "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
        "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
        "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
        "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
        "  ]\n",
        "columns2= [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
        "\n",
        "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
        "\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbdLuSVxTulz",
        "outputId": "71565356-e041-46ba-eb93-1436c188f49b"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- bonus: long (nullable = true)\n",
            "\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|James        |Sales     |NY   |90000 |34 |10000|\n",
            "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
            "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
            "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
            "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#both union and unionall returns the same.\n",
        "\n",
        "df.unionAll(df2).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXD4OylqTxlD",
        "outputId": "4caa78fd-ab90-466c-c337-be6978ba26bb"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#union without duplicates\n",
        "\n",
        "df.union(df2).distinct().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rxAdLc-T2fV",
        "outputId": "f126aff7-bbd9-436a-e95a-a35aecf1fb67"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+-----+------+---+-----+\n",
            "|employee_name|department|state|salary|age|bonus|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "|        James|     Sales|   NY| 90000| 34|10000|\n",
            "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
            "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
            "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
            "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
            "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
            "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
            "+-------------+----------+-----+------+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unionByName()\n",
        "\n",
        "The difference between unionByName() function and union() is that this function\n",
        "resolves columns by name (not by position). In other words, unionByName() is used to merge two DataFrames by column names instead of by position.\n",
        "\n",
        "unionByName() also provides an argument allowMissingColumns to specify if you have a different column counts. In case you are using an older than Spark 3.1 version, use the below approach to merge DataFrames with different column names."
      ],
      "metadata": {
        "id": "YZRukJ26UnIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame df1 with columns name, and id\n",
        "data = [(\"James\",34), (\"Michael\",56), \\\n",
        "        (\"Robert\",30), (\"Maria\",24) ]\n",
        "\n",
        "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
        "df1.printSchema()\n",
        "\n",
        "# Create DataFrame df2 with columns name and id\n",
        "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
        "       (45,\"Jen\"),(34,\"Jeff\")]\n",
        "\n",
        "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgKT3SW0UOpD",
        "outputId": "73462277-b215-4e87-f062-de19d9de2537"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.unionByName(df2,allowMissingColumns=True).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CawneO9IU1pZ",
        "outputId": "d957c6d0-a210-44f9-8865-081b36af9aac"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name| id|\n",
            "+-------+---+\n",
            "|  James| 34|\n",
            "|Michael| 56|\n",
            "| Robert| 30|\n",
            "|  Maria| 24|\n",
            "|  James| 34|\n",
            "|  Maria| 45|\n",
            "|    Jen| 45|\n",
            "|   Jeff| 34|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unionByName with different col names\n",
        "\n",
        "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "df1.unionByName(df2,allowMissingColumns=True).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFxzhptHU9QI",
        "outputId": "a1ff18b4-1f82-4f1c-b074-02cd864b00b8"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+----+\n",
            "|col0|col1|col2|col3|\n",
            "+----+----+----+----+\n",
            "|   5|   2|   6|NULL|\n",
            "|NULL|   6|   7|   3|\n",
            "+----+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBFthkxIVU5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}