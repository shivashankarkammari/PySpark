{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivashankarkammari/PySpark/blob/main/4_DataFrame_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM1Qe-BdYy2B",
        "outputId": "c550f654-9469-413e-ea4f-c7b1ce705d22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctL9xDMTY3do"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DF2\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WGLEpOsZgSk"
      },
      "source": [
        "# transform()\n",
        "\n",
        "DataFrame.transform(func: Callable[[…], DataFrame], *args: Any, **kwargs: Any) → pyspark.sql.dataframe.DataFrame\n",
        "\n",
        "The following are the parameters:\n",
        "\n",
        "func - Custom function to call.\n",
        "\n",
        "*args - Arguments to pass to func.\n",
        "\n",
        "*kwargs - Keyword arguments to pass to func."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL6tGMAbZYwd",
        "outputId": "ab3eabe7-d807-4916-c9cb-1f5034ae92ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n",
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "|Scala     |4500|15      |\n",
            "|PHP       |3000|20      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVNcPSdqZqRP",
        "outputId": "9c89dbf4-b0ad-465d-905c-cedf4744e019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+--------+\n",
            "|CourseName| fee|discount|\n",
            "+----------+----+--------+\n",
            "|      JAVA|4000|       5|\n",
            "|    PYTHON|4600|      10|\n",
            "|     SCALA|4100|      15|\n",
            "|     SCALA|4500|      15|\n",
            "|       PHP|3000|      20|\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#custom transformation\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "def to_upper(df):\n",
        "  return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
        "\n",
        "\n",
        "\n",
        "df = df.transform(to_upper)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O_WBXnSbVBJ",
        "outputId": "89404535-d992-4cf3-fbd1-bf91d4258328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+--------+-------+\n",
            "|CourseName| fee|discount|new_fee|\n",
            "+----------+----+--------+-------+\n",
            "|      JAVA|4000|       5|   3000|\n",
            "|    PYTHON|4600|      10|   3600|\n",
            "|     SCALA|4100|      15|   3100|\n",
            "|     SCALA|4500|      15|   3500|\n",
            "|       PHP|3000|      20|   2000|\n",
            "+----------+----+--------+-------+\n",
            "\n",
            "+----------+----+--------+-------+---------+\n",
            "|CourseName| fee|discount|new_fee|Final_fee|\n",
            "+----------+----+--------+-------+---------+\n",
            "|      JAVA|4000|       5|   3000|   2850.0|\n",
            "|    PYTHON|4600|      10|   3600|   3240.0|\n",
            "|     SCALA|4100|      15|   3100|   2635.0|\n",
            "|     SCALA|4500|      15|   3500|   2975.0|\n",
            "|       PHP|3000|      20|   2000|   1600.0|\n",
            "+----------+----+--------+-------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#reducing the fee with custom transformation\n",
        "\n",
        "def reduce_fee(df,reduce_amount):\n",
        "  return df.withColumn(\"new_fee\",df.fee - reduce_amount)\n",
        "\n",
        "df = df.transform(reduce_fee,1000)\n",
        "df.show()\n",
        "\n",
        "\n",
        "#applying the discount with custom transformations\n",
        "\n",
        "def discount(df):\n",
        "  return df.withColumn(\"Final_fee\",(df.new_fee - (df.new_fee*(df.discount/100)) ))\n",
        "\n",
        "df = df.transform(discount)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws2zXt1ldSNM"
      },
      "source": [
        "The PySpark sql.functions.transform() is used to apply the transformation on a column of type Array. This function applies the specified transformation on every element of the array and returns an object of ArrayType."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pFnWV_mcEjz",
        "outputId": "f98efd7f-2bb2-4562-90f1-9bb9e1d673d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Languages1: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- Languages2: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+------------------+---------------+\n",
            "|            Name|        Languages1|     Languages2|\n",
            "+----------------+------------------+---------------+\n",
            "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
            "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
            "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
            "+----------------+------------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [\n",
        " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
        "]\n",
        "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUZKBR1VdZum",
        "outputId": "cf2307de-1ced-4f47-e3d2-25adc49f9e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+\n",
            "|          Lang2|\n",
            "+---------------+\n",
            "|  [SPARK, JAVA]|\n",
            "|  [SPARK, JAVA]|\n",
            "|[SPARK, PYTHON]|\n",
            "+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(transform(\"languages2\",lambda x: upper(x)).alias(\"Lang2\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8Y4bmlseS8V"
      },
      "source": [
        "apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZGOY61ldn9p",
        "outputId": "da8959f3-d802-465b-df46-62da7da3c2c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3ZtT7AmeU5j",
        "outputId": "8cacebf4-b131-45ce-9f89-90f65b727590"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Fee  Discount\n",
            "0  20000.0      1000\n",
            "1  25000.0      2500\n",
            "2  30000.0      1500\n",
            "3  22000.0      1200\n",
            "4      NaN      3000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    21000.0\n",
            "1    27500.0\n",
            "2    31500.0\n",
            "3    23200.0\n",
            "4        NaN\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pyspark.pandas as ps\n",
        "import numpy as np\n",
        "\n",
        "technologies = ({\n",
        "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
        "    'Discount':[1000,2500,1500,1200,3000]\n",
        "               })\n",
        "# Create a DataFrame\n",
        "psdf = ps.DataFrame(technologies)\n",
        "print(psdf)\n",
        "\n",
        "def add(data):\n",
        "   return data[0] + data[1]\n",
        "\n",
        "addDF = psdf.apply(add,axis=1)\n",
        "print(addDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2PXwaq4h8AP"
      },
      "source": [
        "# fillna()\n",
        "\n",
        "DF.fillna(value,subset=[col_name])\n",
        "\n",
        "df.na.fill(value,subset=[col_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-Jc0YWFiUxo"
      },
      "source": [
        "# pivot(), unpivot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHt8hAL0fDa1",
        "outputId": "e22811cf-9097-4f7c-de02-a83c3c2c0af3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Amount: long (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+-------+------+-------+\n",
            "|Product|Amount|Country|\n",
            "+-------+------+-------+\n",
            "|Banana |1000  |USA    |\n",
            "|Carrots|1500  |USA    |\n",
            "|Beans  |1600  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Banana |400   |China  |\n",
            "|Carrots|1200  |China  |\n",
            "|Beans  |1500  |China  |\n",
            "|Orange |4000  |China  |\n",
            "|Banana |2000  |Canada |\n",
            "|Carrots|2000  |Canada |\n",
            "|Beans  |2000  |Mexico |\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
        "\n",
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OLY07UM1iZ8J",
        "outputId": "95a533b2-b70d-43ff-e8e9-8c919b1e0ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Canada: long (nullable = true)\n",
            " |-- China: long (nullable = true)\n",
            " |-- Mexico: long (nullable = true)\n",
            " |-- USA: long (nullable = true)\n",
            "\n",
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico|USA |\n",
            "+-------+------+-----+------+----+\n",
            "|Orange |NULL  |4000 |NULL  |4000|\n",
            "|Beans  |NULL  |1500 |2000  |1600|\n",
            "|Banana |2000  |400  |NULL  |1000|\n",
            "|Carrots|2000  |1200 |NULL  |1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "6B959J4ixdzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapType()\n",
        "\n",
        "functions for maptype are\n",
        "\n",
        "1.explode()\n",
        "\n",
        "2.map_keys()--> gives keys\n",
        "\n",
        "3.map_values()-->gives values"
      ],
      "metadata": {
        "id": "kLjqpCQHxeXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "data =[(1,{\"fname\":\"shiva\",\"lname\":\"shankar\"}),\n",
        "       (2,{\"fname\":\"Hari\",\"lname\":\"prasad\"})]\n",
        "\n",
        "schema = StructType().add(\"id\",IntegerType(),True).add(\"name\",MapType(StringType(),StringType(),True))\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ek_SUHdxgJi",
        "outputId": "af0aba70-ae4d-487f-e182-9f40325b0a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------------------------+\n",
            "|id |name                              |\n",
            "+---+----------------------------------+\n",
            "|1  |{fname -> shiva, lname -> shankar}|\n",
            "|2  |{fname -> Hari, lname -> prasad}  |\n",
            "+---+----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.id,explode(df.name)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZwWZ6s0yNW1",
        "outputId": "12e5c2c9-b862-45a3-8013-5cc96929cd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------+\n",
            "| id|  key|  value|\n",
            "+---+-----+-------+\n",
            "|  1|fname|  shiva|\n",
            "|  1|lname|shankar|\n",
            "|  2|fname|   Hari|\n",
            "|  2|lname| prasad|\n",
            "+---+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df.id,map_keys(df.name)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjp3wCYFzLH6",
        "outputId": "c494266d-a8bb-4925-8570-ebc7827dea46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------+\n",
            "| id|map_keys(name)|\n",
            "+---+--------------+\n",
            "|  1|[fname, lname]|\n",
            "|  2|[fname, lname]|\n",
            "+---+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Aggregate Functions\n",
        "\n",
        "PySpark SQL Aggregate functions are grouped as “agg_funcs” in Pyspark. Below is a list of functions defined under this group. Click on each link to learn with example.\n",
        "\n",
        "approx_count_distinct()-->works similar to distinct.\n",
        "\n",
        "avg()-->gives avg\n",
        "\n",
        "collect_list(col) -->collect_list() function returns all values from an input column with duplicates.\n",
        "\n",
        "collect_set()-->function returns all values from an input column with duplicate values eliminated.\n",
        "\n",
        "countDistinct()-->function returns the number of distinct elements in a columns\n",
        "\n",
        "count()-->function returns number of elements in a column.\n",
        "\n",
        "grouping()-->Indicates whether a given input column is aggregated or not. returns 1 for aggregated or 0 for not aggregated in the result. If you try grouping directly on the salary column you will get below error.\n",
        "\n",
        "first()-->function returns the first element in a column when ignoreNulls is set to true, it returns the first non-null element.\n",
        "\n",
        "last()-->function returns the last element in a column. when ignoreNulls is set to true, it returns the last non-null element.\n",
        "\n",
        "kurtosis()-->function returns the kurtosis of the values in a group.\n",
        "\n",
        "max()-->function returns the maximum value in a column.\n",
        "\n",
        "min()-->function returns the minimum value in a column.\n",
        "\n",
        "mean()-->function returns the average of the values in a column. Alias for Avg\n",
        "\n",
        "skewness-->function returns the skewness of the values in a group.\n",
        "\n",
        "stddev()-->\n",
        "\n",
        "stddev_samp\n",
        "\n",
        "stddev_pop\n",
        "\n",
        "sum()-->function Returns the sum of all values in a column.\n",
        "\n",
        "sumDistinct()-->function returns the sum of all distinct values in a column.\n",
        "\n",
        "variance()-->alias for var_samp()\n",
        "\n",
        "var_samp() function returns the unbiased variance of the values in a column.\n",
        "\n",
        "var_pop() function returns the population variance of the values in a column."
      ],
      "metadata": {
        "id": "logQ8vjY0Otk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK2nMqamzd3N",
        "outputId": "dd51433a-58c2-4ed0-81f7-e1f3c53b4589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#approx_count_distinct\n",
        "\n",
        "df.select(approx_count_distinct(\"salary\").alias(\"unique_salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Foh51JOh0yLv",
        "outputId": "ed76c31d-724e-4bf9-f83c-b8d305fdbd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|unique_salary|\n",
            "+-------------+\n",
            "|            6|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#avg\n",
        "\n",
        "df.select(avg(\"salary\").alias(\"avg_salary\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drT2J5Gq1eCD",
        "outputId": "3072c13a-bb16-4461-d6b9-44225d2f0bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|avg_salary|\n",
            "+----------+\n",
            "|    3400.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window Functions:\n",
        "\n",
        "**row_number(): Column**\tReturns a sequential number starting from 1 within a window partition\n",
        "\n",
        "**rank(): Column**\tReturns the rank of rows within a window partition, with gaps.\n",
        "\n",
        "**percent_rank(): Column**\tReturns the percentile rank of rows within a window partition.\n",
        "\n",
        "**dense_rank(): Column**\tReturns the rank of rows within a window partition without any gaps. Where as Rank() returns rank with gaps.\n",
        "\n",
        "**ntile(n: Int): Column\t**Returns the ntile id in a window partition.\n",
        "\n",
        "**cume_dist(): Column**\tReturns the cumulative distribution of values within a window partition\n",
        "\n",
        "**lag(e: Column, offset: Int): Column**\n",
        "**lag(columnName: String, offset: Int): Column**\n",
        "**lag(columnName: String, offset: Int, defaultValue: Any): Column **\n",
        "returns the value that is `offset` rows before the current row, and `null` if there is less than `offset` rows before the current row.\n",
        "\n",
        "**lead(columnName: String, offset: Int): Column**\n",
        "**lead(columnName: String, offset: Int): Column**\n",
        "**lead(columnName: String, offset: Int, defaultValue: Any): Column**\n",
        "returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row.\n",
        "\n"
      ],
      "metadata": {
        "id": "lrGytDKB_kPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#row_number\n",
        "#row_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.\n",
        "\n",
        "\n",
        "\n",
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNqfFblB2Cvd",
        "outputId": "9a5e1745-b026-4856-84ea-885778aa3e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "m9NXP5_eBzXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowspec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowspec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvJdZ6M6CFSx",
        "outputId": "2931b8d9-6a0c-4196-a2d7-39c785b1fe11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         2|\n",
            "|       Robert|     Sales|  4100|         3|\n",
            "|         Saif|     Sales|  4100|         4|\n",
            "|      Michael|     Sales|  4600|         5|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# import pyspark\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "#     (\"Michael\", \"Sales\", 4600),  \\\n",
        "#     (\"Robert\", \"Sales\", 4100),   \\\n",
        "#     (\"Maria\", \"Finance\", 3000),  \\\n",
        "#     (\"James\", \"Sales\", 3000),    \\\n",
        "#     (\"Scott\", \"Finance\", 3300),  \\\n",
        "#     (\"Jen\", \"Finance\", 3900),    \\\n",
        "#     (\"Jeff\", \"Marketing\", 3000), \\\n",
        "#     (\"Kumar\", \"Marketing\", 2000),\\\n",
        "#     (\"Saif\", \"Sales\", 4100) \\\n",
        "#   )\n",
        "\n",
        "# columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "# df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "\n",
        "# df.printSchema()\n",
        "# df.show(truncate=False)\n",
        "\n",
        "# from pyspark.sql.window import Window\n",
        "# from pyspark.sql.functions import row_number\n",
        "# windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "# df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "#     .show(truncate=False)\n",
        "\n",
        "# from pyspark.sql.functions import rank\n",
        "# df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "#     .show()\n",
        "\n",
        "# from pyspark.sql.functions import dense_rank\n",
        "# df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "#     .show()\n",
        "\n",
        "# from pyspark.sql.functions import percent_rank\n",
        "# df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "#     .show()\n",
        "\n",
        "# from pyspark.sql.functions import ntile\n",
        "# df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
        "#     .show()\n",
        "\n",
        "# from pyspark.sql.functions import cume_dist\n",
        "# df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
        "#    .show()\n",
        "\n",
        "# from pyspark.sql.functions import lag\n",
        "# df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
        "#       .show()\n",
        "\n",
        "# from pyspark.sql.functions import lead\n",
        "# df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
        "#     .show()\n",
        "\n",
        "# windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "# from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
        "# df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "#   .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "#   .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "#   .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "#   .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "#   .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\"
      ],
      "metadata": {
        "id": "g5fhoz6CChIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Date functions:\n",
        "\n",
        "current_date()-->Returns the current date as a date column.\n",
        "\n",
        "to_date()-->Converts the column into `DateType` by casting rules to `DateType`.\n",
        "\n",
        "to_date(column, fmt)\tConverts the column into a `DateType` with a specified format.\n",
        "\n",
        "add_months(Column, numMonths)-->Returns the date that is `numMonths` after `startDate`.\n",
        "\n",
        "date_add(column, days)\n",
        "date_sub(column, days)-->Returns the date that is `days` days after `start`\n",
        "\n",
        "year(column)\tExtracts the year as an integer from a given date/timestamp/string\n",
        "\n",
        "quarter(column)\tExtracts the quarter as an integer from a given date/timestamp/string.\n",
        "\n",
        "month(column)\tExtracts the month as an integer from a given date/timestamp/string\n",
        "\n",
        "dayofweek(column)\tExtracts the day of the week as an integer from a given date/timestamp/string. Ranges from 1 for a Sunday through to 7 for a Saturday.\n"
      ],
      "metadata": {
        "id": "K_3_KjfIJ-xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Timestamp functions\n",
        "\n",
        "\n",
        "\n",
        "current_timestamp ()-->Returns the current timestamp as a timestamp column\n",
        "\n",
        "hour(column)-->Extracts the hours as an integer from a given date/timestamp/string.\n",
        "\n",
        "minute(column)-->Extracts the minutes as an integer from a given date/timestamp/string.\n",
        "\n",
        "second(column)-->Extracts the seconds as an integer from a given date/timestamp/string.\n",
        "\n",
        "to_timestamp(column)-->Converts to a timestamp by casting rules to `TimestampType`.\n",
        "\n",
        "to_timestamp(column, fmt)-->Converts time string with the given pattern to timestamp."
      ],
      "metadata": {
        "id": "qT5lRqwqK98R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CIwf6-D-Dx-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkFNUlPdHzVj624TF4DeG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}